{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOOM Reinforcement Learning Project Using Vizdoom and Double Dueling Deep Q Learning\n",
    "\n",
    "### Authors: Bartosz Dybowski, Piotr Sieczka\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "DDDQN model prepared for scenario 'Defend the center'.\n",
    "\n",
    "### \"Defend the center\" scenario\n",
    "The goal of this scenario is to train the agent to **survive and defend a specific area** while neutralizing threats.\n",
    "\n",
    "- **Map Layout**: The player is placed in the center of a circular arena, surrounded by monsters attacking from all directions.\n",
    "- **Goal**: The agent must shoot monsters to survive as long as possible.\n",
    "- **Reward System**:\n",
    "  - Positive reward for killing monsters.\n",
    "  - Negative reward for taking damage or dying.\n",
    "- **Challenges**: The agent must balance shooting accurately, managing health, and reacting to multiple threats simultaneously.\n",
    "\n",
    "### Actions\n",
    "The agent has **3 possible actions**, such as turning, moving, and shooting:\n",
    "`[turn left, turn right, shoot]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "Solution based on https://gist.github.com/simoninithomas/d6adc6edb0a7f37d6323a5e3d2ab72ec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "!pip install vizdoom\n",
    "!pip install scikit-image==0.19.0\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf                         # Deep Learning library\n",
    "import numpy as np                              # Handle matrices\n",
    "from vizdoom import *                           # Doom Environment\n",
    "\n",
    "import random                                   # Handling random number generation\n",
    "import time                                     # Handling time calculation\n",
    "from skimage import transform                   # Help us to preprocess the frames\n",
    "\n",
    "from collections import deque                   # Ordered collection with ends\n",
    "import matplotlib.pyplot as plt                 # Display graphs\n",
    "\n",
    "import warnings                                 # This ignore all the warning messages that are printed during the training because of skiimage\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tensorflow.keras.layers import Lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set up paths\n",
    "\n",
    "Scenario Configuration and Resources\n",
    "\n",
    "- **`SCENARIO_CONFIG_PATH`**: A `.cfg` file defining scenario rules, such as player settings, victory conditions, and enemy behavior.\n",
    "- **`SCENARIO_WAD_PATH`**: A `.wad` file containing the map layout, textures, and game assets.\n",
    "- **`MODEL_PATH`**: Path of the saved model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCENARIO_CONFIG_PATH = \"C:\\\\Users\\\\barto\\\\OneDrive\\\\Pulpit\\\\DoomRL\\\\scenarios\\\\defend_the_center.cfg\"\n",
    "SCENARIO_WAD_PATH = \"C:\\\\Users\\\\barto\\\\OneDrive\\\\Pulpit\\\\DoomRL\\\\scenarios\\\\defend_the_center.wad\"\n",
    "MODEL_TO_SAVE_PATH = \"C:\\\\Users\\\\barto\\\\OneDrive\\\\Pulpit\\\\DoomRL\\\\models\\\\dddqn\\\\dddqn_model_dtc.ckpt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Environment\n",
    "\n",
    "The Doom environment requires:\n",
    "  - A **configuration file** (`.cfg`): This handles all gameplay options, such as frame size, available actions, and difficulty level.\n",
    "  - A **scenario file** (`.wad`): This generates the map and assets for the chosen scenario (in this case, **Defend the Center**, but feel free to experiment with others).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment():\n",
    "    game = DoomGame()\n",
    "\n",
    "    # Load the correct configuration\n",
    "    game.load_config(SCENARIO_CONFIG_PATH)\n",
    "\n",
    "    # Load the correct scenario\n",
    "    game.set_doom_scenario_path(SCENARIO_WAD_PATH)\n",
    "\n",
    "    game.set_window_visible(False) # no pop out window\n",
    "    game.init()\n",
    "\n",
    "    # Define three possible actions: turn left, turn right, and shoot\n",
    "    possible_actions = [\n",
    "        [1, 0, 0],  # Turn left\n",
    "        [0, 1, 0],  # Turn right\n",
    "        [0, 0, 1]   # Shoot\n",
    "    ]\n",
    "\n",
    "    return game, possible_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "game, possible_actions = create_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocessing functions ⚙️\n",
    "### a) Preprocess frame\n",
    "Preprocessing in order to to reduce the complexity of our states to reduce the computation time needed for training.\n",
    "<br><br>\n",
    "Steps:\n",
    "- Grayscale each of frames because color does not add important information, but this is already done by the config file.\n",
    "- Crop the screen (in our case we remove the roof because it contains no information)\n",
    "- Normalize pixel values\n",
    "- Finally resize the preprocessed frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    preprocess_frame:\n",
    "\n",
    "    1. Take a frame.\n",
    "    2. Resize it:\n",
    "\n",
    "        from\n",
    "        __________________\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |_________________|\n",
    "\n",
    "        to\n",
    "        _____________\n",
    "        |            |\n",
    "        |            |\n",
    "        |            |\n",
    "        |____________|\n",
    "\n",
    "    3. Normalize it.\n",
    "\n",
    "    4. return preprocessed_frame\n",
    "\n",
    "    \"\"\"\n",
    "def preprocess_frame(frame):\n",
    "    # Crop the screen (remove part that contains no information)\n",
    "    # [Up: Down, Left: right]\n",
    "    cropped_frame = frame[15:-5, 20:-20]\n",
    "\n",
    "    # Check if the cropped frame has non-zero dimensions\n",
    "    if cropped_frame.size == 0:\n",
    "        # If the cropped frame has zero dimensions, return a default frame with zeros\n",
    "        return np.zeros((100, 120), dtype=np.float32)\n",
    "\n",
    "    # Normalize Pixel Values\n",
    "    normalized_frame = cropped_frame / 255.0\n",
    "\n",
    "    # Resize\n",
    "    preprocessed_frame = transform.resize(cropped_frame, [100, 120])\n",
    "\n",
    "    return preprocessed_frame # 100 x 120 x 1 frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Stack frames\n",
    "\n",
    "Stacking frames is perfromed because it helps to give have a sense of motion to our Neural Network.\n",
    "\n",
    "- First preprocess frame\n",
    "- Then append the frame to the deque that automatically removes the oldest frame\n",
    "- Finally build the stacked state\n",
    "\n",
    "This is how work stack:\n",
    "- For the first frame -  feed 4 frames.\n",
    "- At each timestep, add the new frame to deque and then stack them to form a new stacked frame.\n",
    "- And so on ...\n",
    "- If done, create a new stack with 4 new frames (because we are in a new episode).\n",
    "\n",
    "Reference:\n",
    "- <a href=\"https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/\">article</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_size = 4 # We stack 4 frames\n",
    "\n",
    "# Initialize deque with zero-images one array for each image\n",
    "stacked_frames  =  deque([np.zeros((100,120), dtype=int) for i in range(stack_size)], maxlen=4)\n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    if state.size == 0:\n",
    "        # Return the existing stacked frames without modification\n",
    "        return np.stack(stacked_frames, axis=2), stacked_frames\n",
    "\n",
    "    # Preprocess frame\n",
    "    frame = preprocess_frame(state)\n",
    "\n",
    "    if is_new_episode:\n",
    "        # Clear our stacked_frames\n",
    "        stacked_frames = deque([np.zeros((100,120), dtype=int) for i in range(stack_size)], maxlen=4)\n",
    "\n",
    "        # Because we're in a new episode, copy the same frame 4x\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "        # Stack the frames\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "\n",
    "    else:\n",
    "        # Append frame to deque, automatically removes the oldest frame\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "        # Build the stacked state (first dimension specifies different frames)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "\n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Set up hyperparameters\n",
    "In this part different hyperparameters are set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL HYPERPARAMETERS\n",
    "state_size = [100, 120, 4]                                      # Our input is a stack of 4 frames hence 100x120x4 (Width, height, channels)\n",
    "action_size = game.get_available_buttons_size()                 # 3 possible actions\n",
    "learning_rate =  0.00025                                        # Alpha (aka learning rate)\n",
    "\n",
    "### TRAINING HYPERPARAMETERS\n",
    "total_episodes = 20                                            # Total episodes for training\n",
    "max_steps = 10000                                               # Max possible steps in an episode\n",
    "batch_size = 64\n",
    "\n",
    "# FIXED Q TARGETS HYPERPARAMETERS\n",
    "max_tau = 10000                                                 # Tau is the C step where target network is updated\n",
    "\n",
    "# EXPLORATION HYPERPARAMETERS for epsilon greedy strategy\n",
    "explore_start = 1.0                                             # exploration probability at start\n",
    "explore_stop = 0.01                                             # minimum exploration probability\n",
    "decay_rate = 0.00005                                            # exponential decay rate for exploration prob\n",
    "\n",
    "# Q LEARNING hyperparameters\n",
    "gamma = 0.95                                                    # Discounting rate\n",
    "\n",
    "### MEMORY HYPERPARAMETERS\n",
    "pretrain_length = 10                                            # Number of experiences stored in the Memory when initialized for the first time\n",
    "memory_size = 100000                                            # Number of experiences the Memory can keep (CPU: 10, GPU: 1000000)\n",
    "\n",
    "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\n",
    "training = True\n",
    "\n",
    "## TURN THIS TO TRUE IF YOU WANT TO RENDER THE ENVIRONMENT\n",
    "episode_render = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Dueling Double Deep Q-learning Neural Network model\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1500/1*FkHqwA2eSGixdS-3dvVoMA.png\" alt=\"Dueling Double Deep Q Learning Model\" />\n",
    "Dueling Double Deep Q-learning model:\n",
    "\n",
    "- Take a stack of 4 frames as input\n",
    "- It passes through 3 convnets\n",
    "- Then it is flatened\n",
    "- Then it is passed through 2 streams\n",
    "    - One that calculates V(s)\n",
    "    - The other that calculates A(s,a)\n",
    "- Finally an agregating layer\n",
    "- It outputs a Q value for each actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\barto\\AppData\\Local\\Temp\\ipykernel_25596\\840545263.py:2: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#tf.compat.v1.disable_eager_execution()\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDDQNNet(tf.keras.Model):\n",
    "    def __init__(self, state_size, action_size, learning_rate, name=\"DDDQNNet\"):\n",
    "        super(DDDQNNet, self).__init__(name=name)\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Define layers\n",
    "        self.conv1 = tf.keras.layers.Conv2D(\n",
    "            filters=32, kernel_size=(8, 8), strides=(4, 4), activation='elu', name=\"conv1\"\n",
    "        )\n",
    "        self.conv2 = tf.keras.layers.Conv2D(\n",
    "            filters=64, kernel_size=(4, 4), strides=(2, 2), activation='elu', name=\"conv2\"\n",
    "        )\n",
    "        self.conv3 = tf.keras.layers.Conv2D(\n",
    "            filters=128, kernel_size=(4, 4), strides=(2, 2), activation='elu', name=\"conv3\"\n",
    "        )\n",
    "        self.flatten = tf.keras.layers.Flatten(name=\"flatten\")\n",
    "        self.value_fc = tf.keras.layers.Dense(512, activation='elu', name=\"value_fc\")\n",
    "        self.value = tf.keras.layers.Dense(1, activation=None, name=\"value\")\n",
    "        self.advantage_fc = tf.keras.layers.Dense(512, activation='elu', name=\"advantage_fc\")\n",
    "        self.advantage = tf.keras.layers.Dense(self.action_size, activation=None, name=\"advantages\")\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = tf.keras.optimizers.RMSprop(learning_rate=self.learning_rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        # Value and Advantage streams\n",
    "        value = self.value(self.value_fc(x))\n",
    "        advantage = self.advantage(self.advantage_fc(x))\n",
    "\n",
    "        # Combine value and advantage streams\n",
    "        q_values = value + (advantage - tf.reduce_mean(advantage, axis=1, keepdims=True))\n",
    "        return q_values\n",
    "\n",
    "    \n",
    "    def compute_loss(self, target_Q, predicted_Q, actions_mb, ISWeights_):\n",
    "        predicted_Q_for_actions = tf.reduce_sum(predicted_Q * actions_mb, axis=1)\n",
    "        td_error = tf.math.squared_difference(target_Q, predicted_Q_for_actions)\n",
    "        return tf.reduce_mean(ISWeights_ * td_error)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the graph\n",
    "#tf.reset_default_graph()\n",
    "\n",
    "# Instantiate the DQNetwork\n",
    "DQNetwork = DDDQNNet(state_size, action_size, learning_rate, name=\"DQNetwork\")\n",
    "\n",
    "# Instantiate the target network\n",
    "TargetNetwork = DDDQNNet(state_size, action_size, learning_rate, name=\"TargetNetwork\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Prioritized Experience Replay\n",
    "\n",
    "Model cannot use a simple array to do that because sampling from it will be not efficient, so a binary tree data type should be used.\n",
    "\n",
    "To summarize:\n",
    "- **Step 1**: We construct a SumTree, which is a Binary Sum tree where leaves contains the priorities and a data array where index points to the index of leaves.\n",
    "    <img src=\"https://cdn-images-1.medium.com/max/1200/1*Go9DNr7YY-wMGdIQ7HQduQ.png\" alt=\"SumTree\"/>\n",
    "    <br><br>\n",
    "    - **def __init__**: Initialize our SumTree data object with all nodes = 0 and data (data array) with all = 0.\n",
    "    - **def add**: add our priority score in the sumtree leaf and experience (S, A, R, S', Done) in data.\n",
    "    - **def update**: we update the leaf priority score and propagate through tree.\n",
    "    - **def get_leaf**: retrieve priority score, index and experience associated with a leaf.\n",
    "    - **def total_priority**: get the root node value to calculate the total priority score of our replay buffer.\n",
    "<br><br>\n",
    "- **Step 2**: We create a Memory object that will contain our sumtree and data.\n",
    "    - **def __init__**: generates our sumtree and data by instantiating the SumTree object.\n",
    "    - **def store**: we store a new experience in our tree. Each new experience will **have priority = max_priority** (and then this priority will be corrected during the training (when we'll calculating the TD error hence the priority score).\n",
    "    - **def sample**:\n",
    "         - First, to sample a minibatch of k size, the range [0, priority_total] is / into k ranges.\n",
    "         - Then a value is uniformly sampled from each range\n",
    "         - We search in the sumtree, the experience where priority score correspond to sample values are retrieved from.\n",
    "         - Then, we calculate IS weights for each minibatch element\n",
    "    - **def update_batch**: update the priorities on the tree\n",
    "\n",
    "### Reference:\n",
    "\n",
    "-  https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.2_Prioritized_Replay_DQN/RL_brain.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree(object):\n",
    "    \"\"\"\n",
    "    This SumTree code is modified version of Morvan Zhou:\n",
    "    https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.2_Prioritized_Replay_DQN/RL_brain.py\n",
    "    \"\"\"\n",
    "    data_pointer = 0\n",
    "\n",
    "    \"\"\"\n",
    "    Here initialize the tree with all nodes = 0, and initialize the data with all values = 0\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity # Number of leaf nodes (final nodes) that contains experiences\n",
    "\n",
    "        # Generate the tree with all nodes values = 0\n",
    "        # To understand this calculation (2 * capacity - 1) look at the schema above\n",
    "        # Remember we are in a binary node (each node has max 2 children) so 2x size of leaf (capacity) - 1 (root node)\n",
    "        # Parent nodes = capacity - 1\n",
    "        # Leaf nodes = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "\n",
    "        \"\"\" tree:\n",
    "            0\n",
    "           / \\\n",
    "          0   0\n",
    "         / \\ / \\\n",
    "        0  0 0  0  [Size: capacity] it's at this line that there is the priorities score (aka pi)\n",
    "        \"\"\"\n",
    "\n",
    "        # Contains the experiences (so the size of data is capacity)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Here we add our priority score in the sumtree leaf and add the experience in data\n",
    "    \"\"\"\n",
    "    def add(self, priority, data):\n",
    "        # Look at what index we want to put the experience\n",
    "        tree_index = self.data_pointer + self.capacity - 1\n",
    "\n",
    "        \"\"\" tree:\n",
    "            0\n",
    "           / \\\n",
    "          0   0\n",
    "         / \\ / \\\n",
    "tree_index  0 0  0  We fill the leaves from left to right\n",
    "        \"\"\"\n",
    "\n",
    "        # Update data frame\n",
    "        self.data[self.data_pointer] = data\n",
    "\n",
    "        # Update the leaf\n",
    "        self.update (tree_index, priority)\n",
    "\n",
    "        # Add 1 to data_pointer\n",
    "        self.data_pointer += 1\n",
    "\n",
    "        if self.data_pointer >= self.capacity:  # If we're above the capacity, you go back to first index (we overwrite)\n",
    "            self.data_pointer = 0\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Update the leaf priority score and propagate the change through tree\n",
    "    \"\"\"\n",
    "    def update(self, tree_index, priority):\n",
    "        # Change = new priority score - former priority score\n",
    "        change = priority - self.tree[tree_index]\n",
    "        self.tree[tree_index] = priority\n",
    "\n",
    "        # then propagate the change through tree\n",
    "        while tree_index != 0:    # this method is faster than the recursive loop in the reference code\n",
    "\n",
    "            \"\"\"\n",
    "            Here we want to access the line above\n",
    "            THE NUMBERS IN THIS TREE ARE THE INDEXES NOT THE PRIORITY VALUES\n",
    "\n",
    "                0\n",
    "               / \\\n",
    "              1   2\n",
    "             / \\ / \\\n",
    "            3  4 5  [6]\n",
    "\n",
    "            If we are in leaf at index 6, we updated the priority score\n",
    "            We need then to update index 2 node\n",
    "            So tree_index = (tree_index - 1) // 2\n",
    "            tree_index = (6-1)//2\n",
    "            tree_index = 2 (because // round the result)\n",
    "            \"\"\"\n",
    "            tree_index = (tree_index - 1) // 2\n",
    "            self.tree[tree_index] += change\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Here we get the leaf_index, priority value of that leaf and experience associated with that index\n",
    "    \"\"\"\n",
    "    def get_leaf(self, v):\n",
    "        \"\"\"\n",
    "        Tree structure and array storage:\n",
    "        Tree index:\n",
    "             0         -> storing priority sum\n",
    "            / \\\n",
    "          1     2\n",
    "         / \\   / \\\n",
    "        3   4 5   6    -> storing priority for experiences\n",
    "        Array type for storing:\n",
    "        [0,1,2,3,4,5,6]\n",
    "        \"\"\"\n",
    "        parent_index = 0\n",
    "\n",
    "        while True: # the while loop is faster than the method in the reference code\n",
    "            left_child_index = 2 * parent_index + 1\n",
    "            right_child_index = left_child_index + 1\n",
    "\n",
    "            # If we reach bottom, end the search\n",
    "            if left_child_index >= len(self.tree):\n",
    "                leaf_index = parent_index\n",
    "                break\n",
    "\n",
    "            else: # downward search, always search for a higher priority node\n",
    "\n",
    "                if v <= self.tree[left_child_index]:\n",
    "                    parent_index = left_child_index\n",
    "\n",
    "                else:\n",
    "                    v -= self.tree[left_child_index]\n",
    "                    parent_index = right_child_index\n",
    "\n",
    "        data_index = leaf_index - self.capacity + 1\n",
    "\n",
    "        return leaf_index, self.tree[leaf_index], self.data[data_index]\n",
    "\n",
    "    @property\n",
    "    def total_priority(self):\n",
    "        return self.tree[0] # Returns the root node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we don't use deque anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):  # stored as ( s, a, r, s_ ) in SumTree\n",
    "    \"\"\"\n",
    "    This SumTree code is modified version and the original code is from:\n",
    "    https://github.com/jaara/AI-blog/blob/master/Seaquest-DDQN-PER.py\n",
    "    \"\"\"\n",
    "    PER_e = 0.01    # Hyperparameter that we use to avoid some experiences to have 0 probability of being taken\n",
    "    PER_a = 0.6     # Hyperparameter that we use to make a tradeoff between taking only exp with high priority and sampling randomly\n",
    "    PER_b = 0.4     # importance-sampling, from initial value increasing to 1\n",
    "\n",
    "    PER_b_increment_per_sampling = 0.001\n",
    "\n",
    "    absolute_error_upper = 1.  # clipped abs error\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        # Making the tree\n",
    "        \"\"\"\n",
    "        Remember that our tree is composed of a sum tree that contains the priority scores at his leaf\n",
    "        And also a data array\n",
    "        We don't use deque because it means that at each timestep our experiences change index by one.\n",
    "        We prefer to use a simple array and to overwrite when the memory is full.\n",
    "        \"\"\"\n",
    "        self.tree = SumTree(capacity)\n",
    "\n",
    "    \"\"\"\n",
    "    Store a new experience in our tree\n",
    "    Each new experience have a score of max_prority (it will be then improved when we use this exp to train our DDQN)\n",
    "    \"\"\"\n",
    "    def store(self, experience):\n",
    "        # Find the max priority\n",
    "        max_priority = np.max(self.tree.tree[-self.tree.capacity:])\n",
    "\n",
    "        # If the max priority = 0 we can't put priority = 0 since this exp will never have a chance to be selected\n",
    "        # So we use a minimum priority\n",
    "        if max_priority == 0:\n",
    "            max_priority = self.absolute_error_upper\n",
    "\n",
    "        self.tree.add(max_priority, experience)   # set the max p for new p\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    - First, to sample a minibatch of k size, the range [0, priority_total] is / into k ranges.\n",
    "    - Then a value is uniformly sampled from each range\n",
    "    - We search in the sumtree, the experience where priority score correspond to sample values are retrieved from.\n",
    "    - Then, we calculate IS weights for each minibatch element\n",
    "    \"\"\"\n",
    "    def sample(self, n):\n",
    "        # Create a sample array that will contains the minibatch\n",
    "        memory_b = []\n",
    "\n",
    "        b_idx, b_ISWeights = np.empty((n,), dtype=np.int32), np.empty((n, 1), dtype=np.float32)\n",
    "\n",
    "        # Calculate the priority segment\n",
    "        # Here, as explained in the paper, we divide the Range[0, ptotal] into n ranges\n",
    "        priority_segment = self.tree.total_priority / n       # priority segment\n",
    "\n",
    "        # Here we increasing the PER_b each time we sample a new minibatch\n",
    "        self.PER_b = np.min([1., self.PER_b + self.PER_b_increment_per_sampling])  # max = 1\n",
    "\n",
    "        # Calculating the max_weight\n",
    "        p_min = np.min(self.tree.tree[-self.tree.capacity:]) / self.tree.total_priority\n",
    "        max_weight = (p_min * n) ** (-self.PER_b)\n",
    "\n",
    "        for i in range(n):\n",
    "            \"\"\"\n",
    "            A value is uniformly sample from each range\n",
    "            \"\"\"\n",
    "            a, b = priority_segment * i, priority_segment * (i + 1)\n",
    "            value = np.random.uniform(a, b)\n",
    "\n",
    "            \"\"\"\n",
    "            Experience that correspond to each value is retrieved\n",
    "            \"\"\"\n",
    "            index, priority, data = self.tree.get_leaf(value)\n",
    "\n",
    "            #P(j)\n",
    "            sampling_probabilities = priority / self.tree.total_priority\n",
    "\n",
    "            #  IS = (1/N * 1/P(i))**b /max wi == (N*P(i))**-b  /max wi\n",
    "            b_ISWeights[i, 0] = np.power(n * sampling_probabilities, -self.PER_b)/ max_weight\n",
    "\n",
    "            b_idx[i]= index\n",
    "\n",
    "            experience = [data]\n",
    "\n",
    "            memory_b.append(experience)\n",
    "\n",
    "        return b_idx, memory_b, b_ISWeights\n",
    "\n",
    "    \"\"\"\n",
    "    Update the priorities on the tree\n",
    "    \"\"\"\n",
    "    def batch_update(self, tree_idx, abs_errors):\n",
    "        abs_errors += self.PER_e  # convert to abs and avoid 0\n",
    "        clipped_errors = np.minimum(abs_errors, self.absolute_error_upper)\n",
    "        ps = np.power(clipped_errors, self.PER_a)\n",
    "\n",
    "        for ti, p in zip(tree_idx, ps):\n",
    "            self.tree.update(ti, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here deal with the empty memory problem: pre-populate memory by taking random actions and storing the experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate memory\n",
    "memory = Memory(memory_size)\n",
    "\n",
    "# Render the environment\n",
    "game.new_episode()\n",
    "\n",
    "for i in range(pretrain_length):\n",
    "    # If it's the first step\n",
    "    if i == 0:\n",
    "        # First we need a state\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "\n",
    "    # Random action\n",
    "    action = random.choice(possible_actions)\n",
    "\n",
    "    # Get the rewards\n",
    "    reward = game.make_action(action)\n",
    "\n",
    "    # Look if the episode is finished\n",
    "    done = game.is_episode_finished()\n",
    "\n",
    "    # If we're dead\n",
    "    if done:\n",
    "        # We finished the episode\n",
    "        next_state = np.zeros(state.shape)\n",
    "\n",
    "        # Add experience to memory\n",
    "        #experience = np.hstack((state, [action, reward], next_state, done))\n",
    "\n",
    "        experience = state, action, reward, next_state, done\n",
    "        memory.store(experience)\n",
    "\n",
    "        # Start a new episode\n",
    "        game.new_episode()\n",
    "\n",
    "        # First we need a state\n",
    "        state = game.get_state().screen_buffer\n",
    "\n",
    "        # Stack the frames\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "\n",
    "    else:\n",
    "        # Get the next state\n",
    "        next_state = game.get_state().screen_buffer\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "\n",
    "        # Add experience to memory\n",
    "        experience = state, action, reward, next_state, done\n",
    "        memory.store(experience)\n",
    "\n",
    "        # Our state is now the next_state\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train Agent\n",
    "\n",
    "Algorithm:\n",
    "<br>\n",
    "* Initialize the weights for DQN\n",
    "* Initialize target value weights w- <- w\n",
    "* Init the environment\n",
    "* Initialize the decay rate (that will use to reduce epsilon) \n",
    "<br><br>\n",
    "* **For** episode to max_episode **do** \n",
    "    * Make new episode\n",
    "    * Set step to 0\n",
    "    * Observe the first state $s_0$\n",
    "    <br><br>\n",
    "    * **While** step < max_steps **do**:\n",
    "        * Increase decay_rate\n",
    "        * With $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s_t,a)$\n",
    "        * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "        * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "        \n",
    "        * Sample random mini-batch from $D$: $<s, a, r, s'>$\n",
    "        * Set target $\\hat{Q} = r$ if the episode ends at $+1$, otherwise set $\\hat{Q} = r + \\gamma Q(s',argmax_{a'}{Q(s', a', w), w^-)}$\n",
    "        * Make a gradient descent step with loss $(\\hat{Q} - Q(s, a))^2$\n",
    "        * Every C steps, reset: $w^- \\leftarrow w$\n",
    "    * **endfor**\n",
    "    <br><br>\n",
    "* **endfor**\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function will do the part\n",
    "With ϵ select a random action atat, otherwise select at=argmaxaQ(st,a)\n",
    "\"\"\"\n",
    "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions, DQNetwork):\n",
    "    exp_exp_tradeoff = np.random.rand()\n",
    "\n",
    "    # Epsilon-greedy strategy\n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "\n",
    "    if explore_probability > exp_exp_tradeoff:\n",
    "        action = random.choice(possible_actions)  # Exploration\n",
    "    else:\n",
    "        # Exploitation\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        Qs = DQNetwork(state)\n",
    "        choice = np.argmax(Qs.numpy())\n",
    "        action = possible_actions[int(choice)]\n",
    "\n",
    "    return action, explore_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target_graph(DQNetwork, TargetNetwork):\n",
    "    for t, e in zip(TargetNetwork.trainable_variables, DQNetwork.trainable_variables):\n",
    "        t.assign(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 68\u001b[0m\n\u001b[0;32m     66\u001b[0m         target_Qs_batch\u001b[38;5;241m.\u001b[39mappend(rewards_mb[i])\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 68\u001b[0m         target \u001b[38;5;241m=\u001b[39m rewards_mb[i] \u001b[38;5;241m+\u001b[39m \u001b[43mgamma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mq_target_next_state\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     69\u001b[0m         target_Qs_batch\u001b[38;5;241m.\u001b[39mappend(target)\n\u001b[0;32m     71\u001b[0m targets_mb \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(target_Qs_batch)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:140\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merror_handler\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    139\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mis_traceback_filtering_enabled\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    141\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    142\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;66;03m# In some very rare cases,\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;66;03m# `is_traceback_filtering_enabled` (from the outer scope) may not be\u001b[39;00m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;66;03m# accessible from inside this function\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:32\u001b[0m, in \u001b[0;36mis_traceback_filtering_enabled\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m _ENABLE_TRACEBACK_FILTERING \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mlocal()\n\u001b[0;32m     27\u001b[0m _EXCLUDED_PATHS \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     28\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;18m__file__\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[0;32m     29\u001b[0m )\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdebugging.is_traceback_filtering_enabled\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_traceback_filtering_enabled\u001b[39m():\n\u001b[0;32m     34\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Check whether traceback filtering is currently enabled.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;03m  See also `tf.debugging.enable_traceback_filtering()` and\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;124;03m    was called).\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m     47\u001b[0m   value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_ENABLE_TRACEBACK_FILTERING, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Checkpoint for saving the model\n",
    "checkpoint = tf.train.Checkpoint(optimizer=DQNetwork.optimizer, model=DQNetwork)\n",
    "\n",
    "if training:\n",
    "    # Initialize the game\n",
    "    game.init()\n",
    "\n",
    "    # List to store rewards from each episode\n",
    "    episode_rewards_list = []\n",
    "\n",
    "    for episode in range(total_episodes):\n",
    "        step, tau, decay_step = 0, 0, 0\n",
    "        game.new_episode()\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "        episode_rewards = []\n",
    "\n",
    "        while step < max_steps:\n",
    "            step += 1\n",
    "            tau += 1\n",
    "            decay_step += 1\n",
    "\n",
    "            # Predict action\n",
    "            action, explore_probability = predict_action(\n",
    "                explore_start, explore_stop, decay_rate, decay_step, state, possible_actions, DQNetwork\n",
    "            )\n",
    "\n",
    "            # Perform the action in the environment\n",
    "            reward = game.make_action(action)\n",
    "            done = game.is_episode_finished()\n",
    "            episode_rewards.append(reward)\n",
    "\n",
    "            if done:\n",
    "                # If the episode is finished, reset the state\n",
    "                next_state = np.zeros((120, 140), dtype=np.int64)\n",
    "                next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                total_reward = np.sum(episode_rewards)\n",
    "                episode_rewards_list.append(total_reward)\n",
    "\n",
    "                print(f\"Episode: {episode}, Total Reward: {total_reward}, Explore P: {explore_probability}\")\n",
    "                break\n",
    "            else:\n",
    "                # Update the next state\n",
    "                next_state = game.get_state().screen_buffer\n",
    "                next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                state = next_state\n",
    "\n",
    "            # Learning\n",
    "            tree_idx, batch, ISWeights_mb = memory.sample(batch_size)\n",
    "            states_mb = np.array([each[0][0] for each in batch], ndmin=3)\n",
    "            actions_mb = np.array([each[0][1] for each in batch])\n",
    "            rewards_mb = np.array([each[0][2] for each in batch])\n",
    "            next_states_mb = np.array([each[0][3] for each in batch], ndmin=3)\n",
    "            dones_mb = np.array([each[0][4] for each in batch])\n",
    "\n",
    "            # Compute Q-values using Double DQN\n",
    "            q_next_state = DQNetwork(next_states_mb)\n",
    "            q_target_next_state = TargetNetwork(next_states_mb)\n",
    "\n",
    "            target_Qs_batch = []\n",
    "            for i in range(len(batch)):\n",
    "                terminal = dones_mb[i]\n",
    "                action = np.argmax(q_next_state[i].numpy())\n",
    "\n",
    "                if terminal:\n",
    "                    target_Qs_batch.append(rewards_mb[i])\n",
    "                else:\n",
    "                    target = rewards_mb[i] + gamma * q_target_next_state[i][action]\n",
    "                    target_Qs_batch.append(target)\n",
    "\n",
    "            targets_mb = np.array(target_Qs_batch)\n",
    "\n",
    "            # Update the model\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = DQNetwork(states_mb)\n",
    "                loss = DQNetwork.compute_loss(targets_mb, predictions, actions_mb, ISWeights_mb)\n",
    "\n",
    "            gradients = tape.gradient(loss, DQNetwork.trainable_variables)\n",
    "            DQNetwork.optimizer.apply_gradients(zip(gradients, DQNetwork.trainable_variables))\n",
    "\n",
    "        # Save model every 10 episodes\n",
    "        if episode % 10 == 0:\n",
    "            checkpoint.save(MODEL_TO_SAVE_PATH)\n",
    "            print(\"Model Saved\")\n",
    "\n",
    "    # Plot episode rewards\n",
    "    if episode_rewards_list:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(episode_rewards_list, label=\"Episode Rewards\")\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "        plt.title('Episode Rewards Over Time')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No rewards to plot. Ensure training has been run.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Watch agent play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model path\n",
    "MODEL_PATH=\"C:\\\\Users\\\\barto\\\\OneDrive\\\\Pulpit\\\\DoomRL\\\\models\\\\dddqn\\\\dddqn_model_dtc.ckpt-10\"\n",
    "\n",
    "# Initialize the game\n",
    "game = DoomGame()\n",
    "\n",
    "# Load the correct configuration and scenario\n",
    "game.load_config(SCENARIO_CONFIG_PATH)\n",
    "game.set_doom_scenario_path(SCENARIO_WAD_PATH)\n",
    "\n",
    "# Set visibility and initialize game\n",
    "game.set_window_visible(True)\n",
    "game.init()\n",
    "\n",
    "# Restore the trained model\n",
    "checkpoint = tf.train.Checkpoint(model=DQNetwork)\n",
    "checkpoint.restore(MODEL_PATH).expect_partial()\n",
    "\n",
    "# Play the game for a few episodes\n",
    "for i in range(10):\n",
    "    game.new_episode()\n",
    "    state = game.get_state().screen_buffer\n",
    "    state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "\n",
    "    while not game.is_episode_finished():\n",
    "        # Epsilon-greedy strategy\n",
    "        exp_exp_tradeoff = np.random.rand()\n",
    "        explore_probability = 0.01  # Fixed low epsilon for testing\n",
    "\n",
    "        if explore_probability > exp_exp_tradeoff:\n",
    "            # Random action (exploration)\n",
    "            action_index = np.random.choice(range(len(possible_actions)))\n",
    "        else:\n",
    "            # Exploit the trained model\n",
    "            Qs = DQNetwork(tf.convert_to_tensor(state.reshape((1, *state.shape)), dtype=tf.float32))\n",
    "            action_index = np.argmax(Qs.numpy())\n",
    "\n",
    "        # Perform the action\n",
    "        action = possible_actions[action_index]\n",
    "        game.make_action(action)\n",
    "\n",
    "        if game.is_episode_finished():\n",
    "            break\n",
    "        else:\n",
    "            # Get the next state\n",
    "            next_state = game.get_state().screen_buffer\n",
    "            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "            state = next_state\n",
    "\n",
    "    # Get the final score for this episode\n",
    "    score = game.get_total_reward()\n",
    "    print(f\"Episode {i + 1} Score: {score}\")\n",
    "\n",
    "# Close the game\n",
    "game.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
